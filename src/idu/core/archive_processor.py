import os
import time
import shutil
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List

from idu.core.archive_handler import ArchiveHandler
from idu.core.json_handler import JsonHandler
from idu.core.path_handler import PathHandler
from idu.core.uuid_handler import UuidHandler
from idu.sql.db_manager import DBManager

from loguru import logger


class ArchiveProcessor:
    """å‹ç¼©æ–‡ä»¶å¤„ç†ç±»"""
    
    def __init__(self, target_directory: str, uuid_directory: str, 
                 max_workers: int = 5, order: str = 'mtime'):
        self.target_directory = target_directory
        self.uuid_directory = uuid_directory
        self.max_workers = max_workers
        self.order = order  # ä¿å­˜æ’åºæ–¹å¼
        self.total_archives = 0  # æ€»æ–‡ä»¶æ•°
        self.processed_archives = 0  # å·²å¤„ç†æ–‡ä»¶æ•°
        self.db_path = os.path.join(self.uuid_directory, 'artworks.db')
        self.uuid_set = self._load_all_uuids()
    
    def _load_all_uuids(self):
        db = DBManager(self.db_path)
        uuid_list = db.get_all_uuids()
        db.close()
        return set(uuid_list)

    def _write_sqlite_and_json(self, uuid, json_data, file_name, artist, relative_path, created_time, bak=None):
        # åªå†™å…¥sqliteï¼Œä¸å†åšjsonå¤‡ä»½ï¼Œæ”¯æŒbak
        if uuid not in self.uuid_set:
            db = DBManager(self.db_path)
            db.insert_or_replace(uuid, json_data, file_name, artist, relative_path, created_time, bak)
            db.close()
            self.uuid_set.add(uuid)

    def process_archives(self) -> bool:
        """å¤„ç†æ‰€æœ‰å‹ç¼©æ–‡ä»¶ï¼ˆSSDä¼˜åŒ–ç‰ˆï¼‰"""
        try:
            timestamp = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
            logger.info("[#current_stats]ğŸ” å¼€å§‹æ‰«æå‹ç¼©æ–‡ä»¶")
            
            # ç›´æ¥å¿«é€Ÿæ‰«æSSD
            archive_files = []
            for root, _, files in os.walk(self.target_directory):
                for file in files:
                    if file.endswith(('.zip', '.rar', '.7z')):
                        archive_files.append(os.path.join(root, file))
            
            self.total_archives = len(archive_files)
            self.processed_archives = 0
            logger.info(f"[#current_stats]å…±å‘ç° {self.total_archives} ä¸ªå‹ç¼©æ–‡ä»¶")
            
            # ä½¿ç”¨å†…å­˜ç¼“å­˜å¤„ç†
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = [executor.submit(self.process_single_archive, path, timestamp) 
                         for path in archive_files]
                
                for future in as_completed(futures):
                    future.result()
                    self.processed_archives += 1
                    progress = (self.processed_archives / self.total_archives) * 100
                    logger.info(f"[@current_progress]å¤„ç†è¿›åº¦: ({self.processed_archives}/{self.total_archives}) {progress:.1f}%")
            
            return True
        finally:
            logger.info("[#current_stats]âœ¨ æ‰€æœ‰æ–‡ä»¶å¤„ç†å®Œæˆï¼")
    
    def process_single_archive(self, archive_path: str, timestamp: str) -> bool:
        """å¤„ç†å•ä¸ªå‹ç¼©æ–‡ä»¶
        
        Args:
            archive_path: å‹ç¼©åŒ…è·¯å¾„
            timestamp: æ—¶é—´æˆ³
            
        Returns:
            bool: å¤„ç†æ˜¯å¦æˆåŠŸ
        """
        try:
            # è·å–æ–‡ä»¶ä¿¡æ¯
            artist_name = PathHandler.get_artist_name(self.target_directory, archive_path, 'multi')  # é»˜è®¤ä¸ºå¤šäººæ¨¡å¼
            archive_name = os.path.basename(archive_path)
            relative_path = PathHandler.get_relative_path(self.target_directory, archive_path)
            
            # æ£€æŸ¥å‹ç¼©åŒ…ä¸­çš„JSONæ–‡ä»¶å’ŒYAMLæ–‡ä»¶
            valid_json_files, yaml_files, all_json_files = self._find_valid_json_files(archive_path)
            
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨é‡åä½†æ—¶é—´æˆ³ä¸åŒçš„JSONæ–‡ä»¶
            json_base_names = {}
            for name, _ in valid_json_files:
                base_name = os.path.splitext(name)[0]
                if base_name in json_base_names:
                    logger.info(f"[#process]å‘ç°é‡åJSONæ–‡ä»¶ï¼Œå°†é‡æ–°ç”Ÿæˆ: {os.path.basename(archive_path)}")
                    return self._handle_multiple_json(archive_path, valid_json_files, yaml_files, all_json_files, archive_name, artist_name, relative_path, timestamp)
                json_base_names[base_name] = True
            
            # å¦‚æœå­˜åœ¨YAMLæ–‡ä»¶ï¼Œéœ€è¦åˆ é™¤å¹¶é‡æ–°ç”ŸæˆJSON
            if yaml_files:
                logger.info(f"[#process]å‘ç°YAMLæ–‡ä»¶ï¼Œå°†åˆ é™¤å¹¶ç”Ÿæˆæ–°JSON: {os.path.basename(archive_path)}")
                return self._handle_multiple_json(archive_path, valid_json_files, yaml_files, all_json_files, archive_name, artist_name, relative_path, timestamp)
            
            # æ ¹æ®JSONæ–‡ä»¶æ•°é‡å†³å®šå¤„ç†æ–¹å¼
            if len(valid_json_files) == 1 and len(all_json_files) == 1:
                return self._handle_single_json(archive_path, valid_json_files[0], archive_name, artist_name, relative_path, timestamp)
            else:
                return self._handle_multiple_json(archive_path, valid_json_files, yaml_files, all_json_files, archive_name, artist_name, relative_path, timestamp)
                
        except Exception as e:
            logger.error(f"[#process]å¤„ç†å‹ç¼©åŒ…æ—¶å‡ºé”™ {archive_path}: {str(e)}")
            return True
    
    def _find_valid_json_files(self, archive_path: str) -> tuple:
        """æŸ¥æ‰¾å‹ç¼©åŒ…ä¸­çš„æœ‰æ•ˆJSONæ–‡ä»¶å’ŒYAMLæ–‡ä»¶
        
        Args:
            archive_path: å‹ç¼©åŒ…è·¯å¾„
            
        Returns:
            tuple: (æœ‰æ•ˆJSONæ–‡ä»¶åˆ—è¡¨[(æ–‡ä»¶å, JSONå†…å®¹)], YAMLæ–‡ä»¶åˆ—è¡¨[æ–‡ä»¶å], æ‰€æœ‰JSONæ–‡ä»¶åˆ—è¡¨[æ–‡ä»¶å])
        """
        import subprocess
        import orjson

        valid_json_files = []
        yaml_files = []
        all_json_files = []  # å­˜å‚¨æ‰€æœ‰JSONæ–‡ä»¶ï¼ŒåŒ…æ‹¬æ— æ•ˆçš„

        try:
            # ä½¿ç”¨7zåˆ—å‡ºæ–‡ä»¶
            result = subprocess.run(
                ['7z', 'l', archive_path],
                capture_output=True,
                text=True,
                encoding='gbk',
                errors='ignore',
                check=True
            )

            # æå–ä¸´æ—¶ç›®å½•
            temp_dir = os.path.join(os.path.dirname(archive_path), '.temp_extract')
            os.makedirs(temp_dir, exist_ok=True)

            try:
                # æå–æ‰€æœ‰JSONå’ŒYAMLæ–‡ä»¶
                subprocess.run(
                    ['7z', 'e', archive_path, '*.json', '*.yaml', f"-o{temp_dir}"],
                    stdout=subprocess.DEVNULL,
                    stderr=subprocess.DEVNULL,
                    check=True
                )

                # å¤„ç†JSONæ–‡ä»¶
                for file in os.listdir(temp_dir):
                    if file.endswith('.json'):
                        all_json_files.append(file)
                        try:
                            with open(os.path.join(temp_dir, file), 'rb') as f:
                                json_content = orjson.loads(f.read())
                                if "uuid" in json_content and "timestamps" in json_content:
                                    valid_json_files.append((file, json_content))
                        except Exception:
                            continue
                    elif file.endswith('.yaml'):
                        yaml_files.append(file)
            finally:
                shutil.rmtree(temp_dir, ignore_errors=True)
        except subprocess.CalledProcessError:
            pass
                
        return valid_json_files, yaml_files, all_json_files
    
    def _handle_single_json(self, archive_path: str, json_file: tuple, archive_name: str, 
                          artist_name: str, relative_path: str, timestamp: str) -> bool:
        """å¤„ç†å•ä¸ªJSONæ–‡ä»¶çš„æƒ…å†µ
        
        Args:
            archive_path: å‹ç¼©åŒ…è·¯å¾„
            json_file: (æ–‡ä»¶å, JSONå†…å®¹)å…ƒç»„
            archive_name: å‹ç¼©åŒ…åç§°
            artist_name: è‰ºæœ¯å®¶åç§°
            relative_path: ç›¸å¯¹è·¯å¾„
            timestamp: æ—¶é—´æˆ³
            
        Returns:
            bool: å¤„ç†æ˜¯å¦æˆåŠŸ
        """
        json_name, json_content = json_file
        # åªæå–æ–‡ä»¶åéƒ¨åˆ†ï¼Œå¿½ç•¥è·¯å¾„
        json_filename = os.path.basename(json_name)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
        if not JsonHandler.check_and_update_record(json_content, archive_name, artist_name, relative_path, timestamp):
            return True
        logger.info(f"[#process]æ£€æµ‹åˆ°è®°å½•éœ€è¦æ›´æ–°: {os.path.basename(archive_path)}")
        old_json = json_content.copy()
        updated_json = JsonHandler.update_record(json_content, archive_name, artist_name, relative_path, timestamp)
        import orjson
        json_str = orjson.dumps(updated_json).decode('utf-8')
        created_time = timestamp
        bak = None
        if updated_json is None or (isinstance(updated_json, dict) and updated_json.get('__merge_failed__')):
            bak = orjson.dumps(old_json).decode('utf-8')
            updated_json = old_json
            json_str = orjson.dumps(updated_json).decode('utf-8')
        # ç›´æ¥ç”¨åˆ†å±‚ç›®å½•ä¿å­˜json
        day_dir = PathHandler.get_uuid_path(self.uuid_directory, timestamp)
        json_path = os.path.join(day_dir, json_filename)  # ä½¿ç”¨æ–‡ä»¶åè€Œä¸æ˜¯åŸå§‹è·¯å¾„
        os.makedirs(os.path.dirname(json_path), exist_ok=True)
        if JsonHandler.save(json_path, updated_json):
            if ArchiveHandler.add_json_to_archive(archive_path, json_path, json_filename):  # ä¼ é€’æ–‡ä»¶å
                logger.info(f"[#update]âœ… å·²æ›´æ–°å‹ç¼©åŒ…ä¸­çš„JSONè®°å½•: {archive_name}")
                uuid = updated_json.get('uuid', '')
                self._write_sqlite_and_json(
                    uuid, json_str, archive_name, artist_name, relative_path, created_time, None if bak is None else bak)
                return True
        return False
    
    def _handle_multiple_json(self, archive_path: str, valid_json_files: List[tuple], yaml_files: List[str], 
                            all_json_files: List[str], archive_name: str, artist_name: str, 
                            relative_path: str, timestamp: str) -> bool:
        """å¤„ç†å¤šä¸ªJSONæ–‡ä»¶æˆ–éœ€è¦ç”Ÿæˆæ–°JSONçš„æƒ…å†µ
        
        Args:
            archive_path: å‹ç¼©åŒ…è·¯å¾„
            valid_json_files: æœ‰æ•ˆçš„JSONæ–‡ä»¶åˆ—è¡¨
            yaml_files: YAMLæ–‡ä»¶åˆ—è¡¨
            all_json_files: æ‰€æœ‰JSONæ–‡ä»¶åˆ—è¡¨
            archive_name: å‹ç¼©åŒ…åç§°
            artist_name: è‰ºæœ¯å®¶åç§°
            relative_path: ç›¸å¯¹è·¯å¾„
            timestamp: æ—¶é—´æˆ³
            
        Returns:
            bool: å¤„ç†æ˜¯å¦æˆåŠŸ
        """
        files_to_delete = all_json_files
        files_to_delete.extend(yaml_files)
        if files_to_delete:
            logger.info(f"[#process]åˆ é™¤ç°æœ‰æ–‡ä»¶: {os.path.basename(archive_path)}")
            try:
                ArchiveHandler.delete_files_from_archive(archive_path, ['*.json', '*.yaml'])
            except Exception:
                pass
            ArchiveHandler.delete_files_from_archive(archive_path, files_to_delete)
        uuid_value = UuidHandler.generate_uuid(UuidHandler.load_existing_uuids(self.db_path))
        json_filename = f"{uuid_value}.json"
        day_dir = PathHandler.get_uuid_path(self.uuid_directory, timestamp)
        json_path = os.path.join(day_dir, json_filename)
        new_record = {
            "archive_name": archive_name,
            "artist_name": artist_name,
            "relative_path": relative_path
        }
        json_data = {
            "uuid": uuid_value,
            "timestamps": {
                timestamp: new_record
            }
        }
        import orjson
        json_str = orjson.dumps(json_data).decode('utf-8')
        created_time = timestamp
        bak = None
        os.makedirs(os.path.dirname(json_path), exist_ok=True)
        if JsonHandler.save(json_path, json_data):
            if ArchiveHandler.add_json_to_archive(archive_path, json_path, json_filename):
                logger.info(f"[#update]âœ… å·²æ·»åŠ æ–°JSONåˆ°å‹ç¼©åŒ…: {archive_name}")
                self._write_sqlite_and_json(
                    uuid_value, json_str, archive_name, artist_name, relative_path, created_time, None if bak is None else bak)
                return True
        else:
            logger.error(f"[#process]æ·»åŠ JSONåˆ°å‹ç¼©åŒ…å¤±è´¥: {archive_name}")
        return False
